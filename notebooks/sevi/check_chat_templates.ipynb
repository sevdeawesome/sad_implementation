{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c36d5b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 22 22:42:49 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   0  NVIDIA H100 NVL                Off |   00000000:61:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             59W /  400W |       0MiB /  95830MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f8767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# models = [\n",
    "#     'Qwen/Qwen2.5-7B-Instruct',\n",
    "#     'Qwen/Qwen2.5-32B-Instruct',\n",
    "#     'Qwen/Qwen3-32B',\n",
    "#     \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# ]\n",
    "\n",
    "# for m in models:\n",
    "#     try:\n",
    "#         tok = AutoTokenizer.from_pretrained(m)\n",
    "#         messages = [{'role': 'user', 'content': 'Hello'}]\n",
    "#         result = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "#         print(f'=== {m} ===')\n",
    "#         print(result)\n",
    "#         print()\n",
    "#     except Exception as e:\n",
    "#         print(f'{m}: {e}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ca49a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c1071ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Qwen/Qwen2.5-32B-Instruct ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59318c978b546268e32ac4d50e3dfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Qwen, a large language model created by Alibaba Cloud to assist and communicate with users, providing help on a wide range of topics and tasks. How can I assist you today?\n",
      "\n",
      "=== Qwen/Qwen3-32B ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a22e7339e5b43228f8458e13492314c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking who I am. I need to introduce myself clearly. First, I should mention my name, Qwen. Then, I should explain my purpose as a large language model developed by Alibaba Cloud's Tongyi Lab. I need to highlight my capabilities, like answering questions, creating text, and supporting multiple languages. Also, I should mention my training data up to 2024 to show my knowledge is current. I should keep the tone friendly and approachable, inviting the user to ask for help. Let me make sure the response is concise but covers all key points without being too technical. Avoid any markdown formatting and keep it natural.\n",
      "</think>\n",
      "\n",
      "Hello! I'm Qwen, a large language model developed by Alibaba Cloud's Tongyi Lab. I'm designed to assist with a wide range of tasks, such as answering questions, creating text (like stories, documents, emails, etc.), and supporting multiple languages. My training data is up to 2024, so I can provide information and insights based on knowledge up to that point. How can I assist you today? ðŸ˜Š\n",
      "\n",
      "=== meta-llama/Meta-Llama-3.1-8B-Instruct ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dcd215af5f641eeabb69b9f16c23585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "models = [\n",
    "    'Qwen/Qwen2.5-32B-Instruct',\n",
    "    'Qwen/Qwen3-32B',\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "]\n",
    "\n",
    "for m in models:\n",
    "    try:\n",
    "        print(f'=== {m} ===')\n",
    "        tok = AutoTokenizer.from_pretrained(m)\n",
    "        model = AutoModelForCausalLM.from_pretrained(m, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "        \n",
    "        messages = [{'role': 'user', 'content': 'Who are you?'}]\n",
    "        input_text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tok(input_text, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        outputs = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "        response = tok.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        print(response)\n",
    "        print()\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f'{m}: {e}')\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e1f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d17c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "severin",
   "language": "python",
   "name": "severin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
